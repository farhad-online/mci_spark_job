import ir.mci.dwbi.bigdata.spark_job.jobs.all_usage.cbs.AllUsageCbsProcessor
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types._

object AllUsageCbsProcessorTest {
  def main(args: Array[String]): Unit = {
    val sparkConf = new org.apache.spark.SparkConf()
      .setAppName("test_all_usage_cbs")

    val spark = SparkSession.builder
      .config(sparkConf)
      .enableHiveSupport()
      .getOrCreate()

    import spark.implicits._

    // Mock Kafka data
    val sampleData = Seq(
      "454000000716673039|0|N|N|202506291330|0|0|0|A|0|20250629133948|20250629133853|20250629133948|14040408170948|14040408170853|14040408170948|D|11006|31|S|100260000252406051|100269450000772371|100260000252405458|10092082934|11000101956714|989936322637|20250601|cirha01.epc.mnc011.mcc432.3gppnetwork.org;1751204326;0;432113980467595,2,229591908,410167488|0|10101|0|10026||1003|0|0|1|1106|16918|17408|2|0|0|0|0|0|0|0|0|0|0|0|11000101956714|100260000252405458|100269450000772371|10092082934|0|0|152525687|20250601|||0|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||620167|0|11000101956714|0|2000|0|152525687|0|0|0|C||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||2|620167|0|16918|1,1656279319327248225,,0;||2|0|0||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||{{{454000007261016330,454000000716673039,0,100260000252405458,701000000045263406,0,2000,620167,0,1656279319327248225,0,0,0,0,0,0,0,0,0,0,1069,1069,C,100269450000772371,20250601,20250601,1.0000}}}|||{{{S,100260000252406051,4002,18432,17408,20250629105011,20250630105011,1106}}}||||||||{{{2,2,000000000000000000000000,000000000000000000000000,,,20220111131643,,,,20250927202959,,20251226202959,,20260922202959,,,,,,,,,,}}}|989936322637|child.internet.mci||432113980467595|10.71.144.64|10.134.66.30|10.71.144.64|432116024133164|20250629133948|3|16918|3978|12940|0|||||||862976182||0|1|300068|0|1|0|98|999|1|98|999|1||1|||20250622000000|1656279319327248225|1000000|1|0|96|0|0|0|100.75.175.13|0|0|0|1361||||0|||||data_107_345_10101_14040408170028_49782.unl"
    )

    val inputSchema = StructType(Seq(
      StructField("value", StringType)
    ))

    val inputDF = spark.createDataFrame(
      sampleData.map(Tuple1(_)).toDF("value").rdd,
      inputSchema
    )

    val processedDF = AllUsageCbsProcessor.process(inputDF)
    processedDF.show()
  }
}